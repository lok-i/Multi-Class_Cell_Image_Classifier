{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "puK1m1TJfQr1",
    "outputId": "f69ad62f-b902-47d6-9ac1-811642d05241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h04W0UcP4uz4",
    "outputId": "bce6b180-5480-4e4a-b267-2b0c221f00d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "import csv\n",
    "import time\n",
    "from scipy import ndimage\n",
    "from tensorflow.keras import callbacks\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "TRAIN_DIR = 'createdData/'\n",
    "IMG_SIZE = 300\n",
    "LR = 0.001\n",
    "training_data =[]\n",
    "labelz = dict(enumerate(['1', '2','3','4','5','6','7','8','9','10','11','12','13','14']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E_5PmkW5JkE"
   },
   "outputs": [],
   "source": [
    "# training_data = np.load('training_data.npy', allow_pickle=True)\n",
    "Gpath='gdrive/My Drive/'\n",
    "training_data = np.load(Gpath+'training_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZiCGR634ywP"
   },
   "outputs": [],
   "source": [
    "def showImg(data):\n",
    "    for img in data:\n",
    "        key = cv2.waitKey(0)\n",
    "        cv2.imshow(str(img[1]),img[0])\n",
    "        print(img[1])\n",
    "        if(key == ord('q')): break\n",
    "\n",
    "def imchange2(mask):\n",
    "    mask = cv2.resize(mask, ( mask.shape[1], int(mask.shape[0]*2)))\n",
    "    if mask.shape[0] >= mask.shape[1]:\n",
    "        j = int(IMG_SIZE * mask.shape[1] / mask.shape[0])\n",
    "        i = IMG_SIZE\n",
    "    else:\n",
    "        i = int(IMG_SIZE * mask.shape[0] / mask.shape[1])\n",
    "        j = IMG_SIZE\n",
    "    mask = cv2.resize(mask, (j, i))\n",
    "    temp = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n",
    "    yi = np.bincount(mask.reshape(-1))\n",
    "    max_color = np.where(yi == max(yi[:-1]))[0][0]\n",
    "    temp+=255\n",
    "    if i == IMG_SIZE:\n",
    "        jt = int((IMG_SIZE - j) / 2)\n",
    "        temp[0:i, jt:jt + j] = mask\n",
    "    else:\n",
    "        it = int((IMG_SIZE - i) / 2)\n",
    "        temp[it:it + i, 0:j] = mask\n",
    "    return cv2.bitwise_not(temp)\n",
    "\n",
    "def create_train_data(N):\n",
    "  for img in tqdm(os.listdir(TRAIN_DIR+labelz[N])):\n",
    "      path = os.path.join(TRAIN_DIR+labelz[N], img)\n",
    "      img_data = cv2.imread(path,0)\n",
    "      img_data = img_data.reshape((1,)+img_data.shape+(1,))\n",
    "      i=0\n",
    "      for batch in datagen.flow(img_data):\n",
    "        if(i>4): break;\n",
    "        i+=1\n",
    "        batch = batch.astype(np.uint8)\n",
    "        batch = batch.reshape(batch.shape[1], batch.shape[2])\n",
    "        final_img = imchange2(batch) \n",
    "        training_data.append([np.array(final_img), str(N)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "DEwg7--P42Sv",
    "outputId": "c47415bb-b862-4f6b-bdea-6b1ab8ed7d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling\n",
      "Done Shuffling!!!\n",
      "10600\n",
      "(7950, 300, 300, 1) (7950, 14)\n",
      "(2650, 300, 300, 1) (2650, 14)\n"
     ]
    }
   ],
   "source": [
    "# training_data = np.load('training_data.npy', allow_pickle=True)\n",
    "print('Shuffling')\n",
    "np.random.shuffle(training_data)\n",
    "print('Done Shuffling!!!')\n",
    "print(training_data.shape[0])\n",
    "# showImg(training_data)\n",
    "\n",
    "\n",
    "train = training_data[:-int(training_data.shape[0]*0.25)]\n",
    "test = training_data[-int(training_data.shape[0]*0.25):]\n",
    "\n",
    "X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train=to_categorical([int(i[1]) for i in train])\n",
    "X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test=to_categorical([int(i[1]) for i in test])\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xVPUuGBR44nA",
    "outputId": "80b19dac-b28a-44f6-d67c-2e1fc6a189e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 10:53:08.103252 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0825 10:53:08.147047 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0825 10:53:08.153720 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0825 10:53:08.189512 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0825 10:53:08.190692 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0825 10:53:10.998471 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0825 10:53:11.060209 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0825 10:53:11.067059 140011952609152 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 96)        11712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 73, 73, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 36, 36, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 17, 17, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 17, 17, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 17, 17, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 17, 17, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 5,518,670\n",
      "Trainable params: 5,515,406\n",
      "Non-trainable params: 3,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = []\n",
    "model = Sequential()\n",
    "model.add(Conv2D(96, (11, 11),strides=(4, 4), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides = 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides = 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides = 2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(14, activation='softmax')) # 2 because we have cat and dog classes\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "uc62cHUr0t7D",
    "outputId": "13ccc74f-74c9-4f04-ed82-d94b3fc9ae64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 10:53:43.300433 140011952609152 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0825 10:53:43.392275 140011952609152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7950 samples, validate on 2650 samples\n",
      "Epoch 1/20\n",
      "7950/7950 [==============================] - 27s 3ms/step - loss: 2.6106 - acc: 0.1531 - val_loss: 2.4057 - val_acc: 0.1940\n",
      "Epoch 2/20\n",
      "7950/7950 [==============================] - 21s 3ms/step - loss: 2.4503 - acc: 0.1813 - val_loss: 2.4412 - val_acc: 0.1811\n",
      "Epoch 3/20\n",
      "7950/7950 [==============================] - 21s 3ms/step - loss: 2.4438 - acc: 0.1706 - val_loss: 3.3818 - val_acc: 0.1913\n",
      "Epoch 4/20\n",
      "7950/7950 [==============================] - 21s 3ms/step - loss: 2.3316 - acc: 0.2038 - val_loss: 2.4712 - val_acc: 0.1834\n",
      "Epoch 5/20\n",
      "7950/7950 [==============================] - 21s 3ms/step - loss: 2.2293 - acc: 0.2322 - val_loss: 2.1610 - val_acc: 0.2623\n",
      "Epoch 6/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 2.1341 - acc: 0.2694 - val_loss: 1.8146 - val_acc: 0.3883\n",
      "Epoch 7/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.9251 - acc: 0.3206 - val_loss: 1.6137 - val_acc: 0.4657\n",
      "Epoch 8/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.8111 - acc: 0.3645 - val_loss: 1.5845 - val_acc: 0.4143\n",
      "Epoch 9/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.6896 - acc: 0.4040 - val_loss: 1.4731 - val_acc: 0.4985\n",
      "Epoch 10/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.6221 - acc: 0.4253 - val_loss: 1.3096 - val_acc: 0.5558\n",
      "Epoch 11/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.5453 - acc: 0.4462 - val_loss: 1.4177 - val_acc: 0.5053\n",
      "Epoch 12/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.4117 - acc: 0.4946 - val_loss: 1.1881 - val_acc: 0.5921\n",
      "Epoch 13/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.3006 - acc: 0.5343 - val_loss: 1.0858 - val_acc: 0.5977\n",
      "Epoch 14/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.2605 - acc: 0.5508 - val_loss: 1.9380 - val_acc: 0.4800\n",
      "Epoch 15/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.1714 - acc: 0.5875 - val_loss: 1.4329 - val_acc: 0.5226\n",
      "Epoch 16/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.1416 - acc: 0.6036 - val_loss: 1.0087 - val_acc: 0.6517\n",
      "Epoch 17/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 1.0443 - acc: 0.6235 - val_loss: 1.0194 - val_acc: 0.6936\n",
      "Epoch 18/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 0.9887 - acc: 0.6577 - val_loss: 0.9196 - val_acc: 0.7113\n",
      "Epoch 19/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 0.8905 - acc: 0.6932 - val_loss: 0.8671 - val_acc: 0.7219\n",
      "Epoch 20/20\n",
      "7950/7950 [==============================] - 22s 3ms/step - loss: 0.8390 - acc: 0.7141 - val_loss: 0.6827 - val_acc: 0.8038\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=LR)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = opt, metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=25,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"VISTA.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "K1KhyUdpcA_w",
    "outputId": "1116de49-c35e-4d49-bc01-8967c287d342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7950 samples, validate on 2650 samples\n",
      "Epoch 1/10\n",
      "7950/7950 [==============================] - 23s 3ms/step - loss: 0.5016 - acc: 0.8530 - val_loss: 0.4613 - val_acc: 0.8668\n",
      "Epoch 2/10\n",
      "7950/7950 [==============================] - 19s 2ms/step - loss: 0.4197 - acc: 0.8717 - val_loss: 0.3296 - val_acc: 0.8989\n",
      "Epoch 3/10\n",
      "7950/7950 [==============================] - 20s 2ms/step - loss: 0.3958 - acc: 0.8809 - val_loss: 0.4316 - val_acc: 0.8951\n",
      "Epoch 4/10\n",
      "7950/7950 [==============================] - 20s 2ms/step - loss: 0.3533 - acc: 0.8908 - val_loss: 0.5627 - val_acc: 0.8257\n",
      "Epoch 5/10\n",
      "7950/7950 [==============================] - 20s 2ms/step - loss: 0.3384 - acc: 0.8991 - val_loss: 3.7463 - val_acc: 0.6019\n",
      "Epoch 6/10\n",
      "7950/7950 [==============================] - 20s 2ms/step - loss: 0.3690 - acc: 0.8940 - val_loss: 0.5320 - val_acc: 0.8509\n",
      "Epoch 7/10\n",
      "7950/7950 [==============================] - 20s 3ms/step - loss: 0.3321 - acc: 0.9001 - val_loss: 0.4083 - val_acc: 0.8943\n",
      "Epoch 8/10\n",
      "7950/7950 [==============================] - 20s 3ms/step - loss: 0.2924 - acc: 0.9109 - val_loss: 0.6463 - val_acc: 0.8226\n",
      "Epoch 9/10\n",
      "7950/7950 [==============================] - 20s 3ms/step - loss: 0.3044 - acc: 0.9122 - val_loss: 0.5242 - val_acc: 0.8592\n",
      "Epoch 10/10\n",
      "7950/7950 [==============================] - 20s 3ms/step - loss: 0.2473 - acc: 0.9248 - val_loss: 0.3834 - val_acc: 0.9087\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=LR)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = opt, metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=30,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"VISTA.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "6QuHXr7ZLvYp",
    "outputId": "9af33e4d-4d59-49fb-b6d5-b445852c9b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/79 [==============================] - 48s 602ms/step - loss: 0.2035 - acc: 0.9446 - val_loss: 0.1227 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12267, saving model to bestModel.model\n",
      "Epoch 2/10\n",
      "80/79 [==============================] - 38s 469ms/step - loss: 0.2117 - acc: 0.9418 - val_loss: 0.1149 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12267 to 0.11487, saving model to bestModel.model\n",
      "Epoch 3/10\n",
      "80/79 [==============================] - 40s 505ms/step - loss: 0.1978 - acc: 0.9449 - val_loss: 0.3157 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11487\n",
      "Epoch 4/10\n",
      "80/79 [==============================] - 40s 496ms/step - loss: 0.2005 - acc: 0.9450 - val_loss: 0.2254 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11487\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 5/10\n",
      "80/79 [==============================] - 38s 474ms/step - loss: 0.1866 - acc: 0.9486 - val_loss: 0.1038 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11487 to 0.10382, saving model to bestModel.model\n",
      "Epoch 6/10\n",
      "80/79 [==============================] - 40s 499ms/step - loss: 0.1738 - acc: 0.9505 - val_loss: 0.1152 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10382\n",
      "Epoch 7/10\n",
      "80/79 [==============================] - 41s 511ms/step - loss: 0.1960 - acc: 0.9487 - val_loss: 0.1328 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10382\n",
      "Epoch 8/10\n",
      "80/79 [==============================] - 40s 499ms/step - loss: 0.1779 - acc: 0.9509 - val_loss: 0.1036 - val_acc: 0.9657\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10382 to 0.10357, saving model to bestModel.model\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/10\n",
      "80/79 [==============================] - 40s 496ms/step - loss: 0.1787 - acc: 0.9516 - val_loss: 0.1074 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10357\n",
      "Epoch 10/10\n",
      "80/79 [==============================] - 40s 494ms/step - loss: 0.1687 - acc: 0.9510 - val_loss: 0.1056 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b870717f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earlystop = EarlyStopping(patience=10,verbose=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "patience=3 ,\n",
    "verbose=1, \n",
    "factor=0.5, \n",
    "min_lr=0.00001)\n",
    "# model = load_model('VISTAfinal.model')\n",
    "save_model = ModelCheckpoint('bestModel.model', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "callbacks = [save_model, learning_rate_reduction]\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = opt, metrics = [\"accuracy\"])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=4,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "#         brightness_range = [0.2,1],\n",
    "        fill_mode='nearest')\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=100),\n",
    "                    steps_per_epoch=len(X_train)/100,epochs=10, callbacks = callbacks,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S71PhU4Fs9G9"
   },
   "outputs": [],
   "source": [
    "Gpath='gdrive/My Drive/'\n",
    "test_data = np.load(Gpath+'eee', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xtyKiIKNgBL"
   },
   "outputs": [],
   "source": [
    "! cp VISTA.model gdrive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8T7fI6mt2Yc"
   },
   "outputs": [],
   "source": [
    "model.save('VISTAfinal.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "YUWzNJlfzOvW",
    "outputId": "eeab4970-c1d6-4db6-a2e1-57c337b4b0e7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41aa40f467c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bestModel.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('bestModel.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48IVAYO_0D56"
   },
   "outputs": [],
   "source": [
    "!cp VISTAfinal.model gdrive/My\\ Drive/VISTAfinal.model\n",
    "!cp bestModel.model gdrive/My\\ Drive/bestModel.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLeMzG4EuJn5"
   },
   "outputs": [],
   "source": [
    "model = load_model('gdrive/My Drive/VISTAfinal.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1uhUxZb3MnF"
   },
   "outputs": [],
   "source": [
    "X_test = np.array([i[0] for i in test_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "name = np.array([i[1] for i in test_data])\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = X_test/255\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "with open('result12.csv', 'a') as csvFile:\n",
    "\twriter = csv.writer(csvFile)\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tclass_prediction = model.predict_classes(X_test[i].reshape(1, IMG_SIZE, IMG_SIZE, 1), verbose = False)\n",
    "\t\trow = [name[i], class_prediction[0]+1]\n",
    "\t\twriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vehoKaXH3ZkF"
   },
   "outputs": [],
   "source": [
    "! cp result12.csv gdrive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFTQcNezUOa7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of VISTA82.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
